{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Exploratory Data Analysis and Data Preprocessing\n",
        "\n",
        "## Objective\n",
        "Understand the structure, content, and quality of the CFPB complaint data and prepare it for the RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: C:\\Users\\USER\\Desktop\\Week7\\Intelligent-Complaint-Analysis-for-Financial-Services\\notebooks\n",
            "Project root: C:\\Users\\USER\\Desktop\\Week7\\Intelligent-Complaint-Analysis-for-Financial-Services\n",
            "Data raw directory: C:\\Users\\USER\\Desktop\\Week7\\Intelligent-Complaint-Analysis-for-Financial-Services\\data\\raw\n",
            "Data raw exists: True\n",
            "Data processed directory: C:\\Users\\USER\\Desktop\\Week7\\Intelligent-Complaint-Analysis-for-Financial-Services\\data\\processed\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Set up paths - more robust path resolution\n",
        "# Try to find project root by looking for data directory\n",
        "current_dir = Path().resolve()\n",
        "project_root = current_dir\n",
        "\n",
        "# Check if we're in the project root (has data/raw directory)\n",
        "if (current_dir / \"data\" / \"raw\").exists():\n",
        "    project_root = current_dir\n",
        "# Check if we're in notebooks directory (go up one level)\n",
        "elif (current_dir.parent / \"data\" / \"raw\").exists():\n",
        "    project_root = current_dir.parent\n",
        "# Otherwise, try going up two levels (if run from notebooks subdirectory)\n",
        "elif (current_dir.parent.parent / \"data\" / \"raw\").exists():\n",
        "    project_root = current_dir.parent.parent\n",
        "else:\n",
        "    # Fallback: assume current directory is project root\n",
        "    project_root = current_dir\n",
        "\n",
        "PROJECT_ROOT = project_root\n",
        "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
        "\n",
        "# Create processed directory if it doesn't exist\n",
        "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Current working directory: {current_dir}\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Data raw directory: {DATA_RAW}\")\n",
        "print(f\"Data raw exists: {DATA_RAW.exists()}\")\n",
        "print(f\"Data processed directory: {DATA_PROCESSED}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load the CFPB Complaint Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found CSV file: C:\\Users\\USER\\Desktop\\Week7\\Intelligent-Complaint-Analysis-for-Financial-Services\\data\\raw\\complaints.csv\n",
            "File size: 5.63 GB\n",
            "Large file detected. Using optimized chunked loading...\n",
            "This may take 5-15 minutes depending on your system...\n",
            "------------------------------------------------------------\n",
            "  Processed 45 chunks (~4,500,000 rows loaded)\n",
            "\n",
            "⚠ Loading interrupted by user.\n",
            "Partial data may be available. Re-run this cell to load the full dataset.\n",
            "\n",
            "⚠ Warning: DataFrame is empty. Please ensure the data file is in data/raw/\n"
          ]
        }
      ],
      "source": [
        "# Load the full CFPB complaint dataset\n",
        "# Note: Update the filename based on the actual dataset file\n",
        "# Common filenames: complaints.csv, consumer_complaints.csv, etc.\n",
        "\n",
        "# Initialize df as empty DataFrame first (in case of errors)\n",
        "df = pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    csv_files = list(DATA_RAW.glob(\"*.csv\"))\n",
        "    if csv_files:\n",
        "        data_file = csv_files[0]\n",
        "        file_size_gb = data_file.stat().st_size / (1024**3)\n",
        "        print(f\"Found CSV file: {data_file}\")\n",
        "        print(f\"File size: {file_size_gb:.2f} GB\")\n",
        "        \n",
        "        # For very large files (>2GB), use optimized chunked reading\n",
        "        if file_size_gb > 2:\n",
        "            print(\"Large file detected. Using optimized chunked loading...\")\n",
        "            print(\"This may take 5-15 minutes depending on your system...\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            # Use larger chunks and process more efficiently\n",
        "            chunk_size = 100000  # Read 100k rows at a time (larger chunks = fewer concatenations)\n",
        "            chunks = []\n",
        "            total_rows = 0\n",
        "            \n",
        "            try:\n",
        "                # First pass: count total rows and load chunks\n",
        "                chunk_reader = pd.read_csv(data_file, low_memory=False, chunksize=chunk_size)\n",
        "                \n",
        "                for i, chunk in enumerate(chunk_reader):\n",
        "                    chunks.append(chunk)\n",
        "                    total_rows += len(chunk)\n",
        "                    \n",
        "                    # Show progress every 5 chunks\n",
        "                    if (i + 1) % 5 == 0:\n",
        "                        print(f\"  Processed {i + 1} chunks (~{total_rows:,} rows loaded)\", end='\\r')\n",
        "                \n",
        "                print(f\"\\n  Loaded {len(chunks)} chunks with {total_rows:,} total rows\")\n",
        "                print(\"  Combining chunks (this may take a few minutes)...\")\n",
        "                \n",
        "                # More memory-efficient concatenation: combine in batches\n",
        "                # Combine chunks in groups to reduce memory spikes\n",
        "                batch_size = 20  # Combine 20 chunks at a time\n",
        "                combined_chunks = []\n",
        "                \n",
        "                for i in range(0, len(chunks), batch_size):\n",
        "                    batch = chunks[i:i+batch_size]\n",
        "                    combined = pd.concat(batch, ignore_index=True)\n",
        "                    combined_chunks.append(combined)\n",
        "                    # Free memory\n",
        "                    del batch\n",
        "                    if (i // batch_size + 1) % 5 == 0:\n",
        "                        print(f\"  Combined {min(i+batch_size, len(chunks))}/{len(chunks)} chunks...\", end='\\r')\n",
        "                \n",
        "                # Final combination\n",
        "                print(f\"\\n  Final combination of {len(combined_chunks)} batches...\")\n",
        "                df = pd.concat(combined_chunks, ignore_index=True)\n",
        "                \n",
        "                # Clean up\n",
        "                del chunks, combined_chunks\n",
        "                import gc\n",
        "                gc.collect()\n",
        "                \n",
        "            except MemoryError:\n",
        "                print(\"\\n⚠ Memory error during chunked loading.\")\n",
        "                print(\"Trying with smaller chunks...\")\n",
        "                # Fallback: smaller chunks\n",
        "                chunk_list = []\n",
        "                for i, chunk in enumerate(pd.read_csv(data_file, low_memory=False, chunksize=25000)):\n",
        "                    chunk_list.append(chunk)\n",
        "                    if (i + 1) % 20 == 0:\n",
        "                        print(f\"  Loaded {i + 1} chunks...\", end='\\r')\n",
        "                print(f\"\\n  Combining {len(chunk_list)} chunks...\")\n",
        "                df = pd.concat(chunk_list, ignore_index=True)\n",
        "                del chunk_list\n",
        "                gc.collect()\n",
        "        else:\n",
        "            print(\"Loading data (this may take a few minutes)...\")\n",
        "            df = pd.read_csv(data_file, low_memory=False)\n",
        "        \n",
        "        print(f\"\\n✓ Data loaded successfully!\")\n",
        "        print(f\"✓ Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "        print(f\"✓ Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "        \n",
        "    else:\n",
        "        print(\"⚠ No CSV file found in data/raw directory.\")\n",
        "        print(\"Please download the CFPB complaint dataset and place it in data/raw/\")\n",
        "        print(\"Dataset can be found at: https://www.consumerfinance.gov/data-research/consumer-complaints/\")\n",
        "        print(\"\\nCreated empty DataFrame for demonstration.\")\n",
        "        \n",
        "except MemoryError as e:\n",
        "    print(f\"\\n⚠ Memory error: The file is too large to load into memory.\")\n",
        "    print(\"Consider using a machine with more RAM or processing the data in smaller batches.\")\n",
        "    df = pd.DataFrame()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n⚠ Loading interrupted by user.\")\n",
        "    print(\"Partial data may be available. Re-run this cell to load the full dataset.\")\n",
        "    df = pd.DataFrame()\n",
        "except Exception as e:\n",
        "    print(f\"\\n⚠ Error loading data: {str(e)}\")\n",
        "    print(\"Created empty DataFrame. Please check the file path and try again.\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "# Verify df is defined\n",
        "if 'df' not in globals():\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "if df.empty:\n",
        "    print(\"\\n⚠ Warning: DataFrame is empty. Please ensure the data file is in data/raw/\")\n",
        "else:\n",
        "    print(f\"\\n✓ DataFrame ready with {len(df):,} rows\")\n",
        "    print(\"✓ You can now proceed to the next cells.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initial Data Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Display basic information about the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset Shape:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mdf\u001b[49m.shape)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mColumn Names:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.columns.tolist())\n",
            "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# Display basic information about the dataset\n",
        "# Check if df is defined and not empty\n",
        "try:\n",
        "    # Try to access df\n",
        "    if 'df' not in globals() or not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        print(\"⚠ Error: DataFrame 'df' is not loaded or is empty.\")\n",
        "        print(\"Please run the data loading cell (Cell 4) first and wait for it to complete.\")\n",
        "        print(\"The data file is large (5.63 GB) and may take several minutes to load.\")\n",
        "        print(\"\\nSteps to fix:\")\n",
        "        print(\"1. Go to Cell 4 (Load the CFPB Complaint Dataset)\")\n",
        "        print(\"2. Run Cell 4 and wait for it to finish (look for '✓ DataFrame ready')\")\n",
        "        print(\"3. Then come back and run this cell\")\n",
        "    else:\n",
        "        print(\"Dataset Shape:\", df.shape)\n",
        "        print(\"\\nColumn Names:\")\n",
        "        print(df.columns.tolist())\n",
        "        print(\"\\nFirst few rows:\")\n",
        "        display(df.head())\n",
        "except NameError:\n",
        "    print(\"⚠ Error: DataFrame 'df' is not defined.\")\n",
        "    print(\"Please run Cell 4 (Load the CFPB Complaint Dataset) first.\")\n",
        "    print(\"Make sure Cell 4 completes successfully before running this cell.\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Error: {str(e)}\")\n",
        "    print(\"Please ensure Cell 4 has been run successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠ Error: DataFrame 'df' is not loaded or is empty.\n",
            "Please run the data loading cell (Cell 4) first and wait for it to complete.\n"
          ]
        }
      ],
      "source": [
        "# Data types and missing values\n",
        "# Check if df is defined and not empty\n",
        "if 'df' not in globals() or df.empty:\n",
        "    print(\"⚠ Error: DataFrame 'df' is not loaded or is empty.\")\n",
        "    print(\"Please run the data loading cell (Cell 4) first and wait for it to complete.\")\n",
        "else:\n",
        "    print(\"Data Types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\nMissing Values Percentage:\")\n",
        "    print((df.isnull().sum() / len(df) * 100).round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Identify Key Columns\n",
        "\n",
        "Based on CFPB dataset structure, we need to identify:\n",
        "- Product category column\n",
        "- Consumer complaint narrative column\n",
        "- Other relevant metadata columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Common column names in CFPB dataset\n",
        "# Adjust these based on actual column names in your dataset\n",
        "\n",
        "# Try to identify columns automatically\n",
        "product_col = None\n",
        "narrative_col = None\n",
        "\n",
        "# Common variations of column names in CFPB dataset\n",
        "# Product column is typically named \"Product\" or similar\n",
        "product_keywords = ['product']\n",
        "# Narrative column is typically \"Consumer complaint narrative\" or similar\n",
        "narrative_keywords = ['consumer_complaint_narrative', 'complaint_narrative', 'consumer complaint narrative']\n",
        "\n",
        "# Try exact matches first (most common CFPB column names)\n",
        "if 'Product' in df.columns:\n",
        "    product_col = 'Product'\n",
        "elif 'product' in df.columns:\n",
        "    product_col = 'product'\n",
        "    \n",
        "if 'Consumer complaint narrative' in df.columns:\n",
        "    narrative_col = 'Consumer complaint narrative'\n",
        "elif 'consumer_complaint_narrative' in df.columns:\n",
        "    narrative_col = 'consumer_complaint_narrative'\n",
        "\n",
        "# If exact matches not found, try fuzzy matching\n",
        "if not product_col:\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower().replace('_', ' ').replace('-', ' ')\n",
        "        if any(keyword in col_lower for keyword in product_keywords):\n",
        "            product_col = col\n",
        "            break\n",
        "\n",
        "if not narrative_col:\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower().replace('_', ' ').replace('-', ' ')\n",
        "        if any(keyword in col_lower for keyword in narrative_keywords):\n",
        "            narrative_col = col\n",
        "            break\n",
        "\n",
        "print(f\"Product column: {product_col}\")\n",
        "print(f\"Narrative column: {narrative_col}\")\n",
        "\n",
        "# Display unique products if found\n",
        "if product_col and not df.empty:\n",
        "    print(f\"\\nUnique Products ({df[product_col].nunique()}):\")\n",
        "    print(df[product_col].value_counts())\n",
        "    \n",
        "    # Show sample of products\n",
        "    print(f\"\\nSample product names (first 10):\")\n",
        "    print(df[product_col].unique()[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Distribution Analysis: Products\n",
        "\n",
        "# Analyze distribution of complaints across different products\n",
        "if product_col and not df.empty:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"PRODUCT DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    product_counts = df[product_col].value_counts()\n",
        "    product_percentages = (df[product_col].value_counts(normalize=True) * 100).round(2)\n",
        "    \n",
        "    print(\"\\nComplaints by Product:\")\n",
        "    print(product_counts)\n",
        "    \n",
        "    print(\"\\nPercentage Distribution:\")\n",
        "    print(product_percentages)\n",
        "    \n",
        "    # Visualize product distribution\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    product_counts.plot(kind='bar')\n",
        "    plt.title('Distribution of Complaints by Product', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Product', fontsize=12)\n",
        "    plt.ylabel('Number of Complaints', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Pie chart for better visualization\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    top_products = product_counts.head(10)  # Top 10 products\n",
        "    plt.pie(top_products.values, labels=top_products.index, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title('Top 10 Products by Complaint Volume', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot analyze product distribution - product column or data not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Consumer Complaint Narrative Analysis\n",
        "\n",
        "# Analyze the Consumer complaint narrative field\n",
        "if narrative_col and not df.empty:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"NARRATIVE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Identify complaints with and without narratives\n",
        "    df['has_narrative'] = df[narrative_col].notna() & (df[narrative_col].str.strip() != '')\n",
        "    \n",
        "    print(f\"\\nTotal complaints: {len(df):,}\")\n",
        "    print(f\"Complaints WITH narrative: {df['has_narrative'].sum():,} ({(df['has_narrative'].sum() / len(df) * 100):.2f}%)\")\n",
        "    print(f\"Complaints WITHOUT narrative: {(~df['has_narrative']).sum():,} ({(~df['has_narrative'].sum() / len(df) * 100):.2f}%)\")\n",
        "    \n",
        "    # Calculate word count for narratives\n",
        "    def count_words(text):\n",
        "        if pd.isna(text) or text == '':\n",
        "            return 0\n",
        "        return len(str(text).split())\n",
        "    \n",
        "    df['narrative_word_count'] = df[narrative_col].apply(count_words)\n",
        "    \n",
        "    # Statistics on narrative length\n",
        "    print(\"\\nNarrative Length Statistics (Word Count):\")\n",
        "    print(df[df['has_narrative']]['narrative_word_count'].describe())\n",
        "    \n",
        "    # Visualize narrative length distribution\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    # Subplot 1: Histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    narratives_with_data = df[df['has_narrative']]['narrative_word_count']\n",
        "    plt.hist(narratives_with_data, bins=50, edgecolor='black', alpha=0.7)\n",
        "    plt.title('Distribution of Narrative Word Counts', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Word Count', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.axvline(narratives_with_data.median(), color='r', linestyle='--', \n",
        "                label=f'Median: {narratives_with_data.median():.0f} words')\n",
        "    plt.axvline(narratives_with_data.mean(), color='g', linestyle='--', \n",
        "                label=f'Mean: {narratives_with_data.mean():.0f} words')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    \n",
        "    # Subplot 2: Box plot (zoomed in for readability)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Filter out extreme outliers for better visualization\n",
        "    q1 = narratives_with_data.quantile(0.25)\n",
        "    q3 = narratives_with_data.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 3 * iqr  # More lenient than standard 1.5*IQR\n",
        "    upper_bound = q3 + 3 * iqr\n",
        "    filtered_narratives = narratives_with_data[(narratives_with_data >= lower_bound) & \n",
        "                                                (narratives_with_data <= upper_bound)]\n",
        "    plt.boxplot(filtered_narratives, vert=True)\n",
        "    plt.title('Narrative Word Count (Outliers Filtered)', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Word Count', fontsize=12)\n",
        "    plt.grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Identify very short and very long narratives\n",
        "    short_threshold = 10  # Less than 10 words\n",
        "    long_threshold = 1000  # More than 1000 words\n",
        "    \n",
        "    short_narratives = df[df['narrative_word_count'] < short_threshold]\n",
        "    long_narratives = df[df['narrative_word_count'] > long_threshold]\n",
        "    \n",
        "    print(f\"\\nVery short narratives (< {short_threshold} words): {len(short_narratives):,}\")\n",
        "    print(f\"Very long narratives (> {long_threshold} words): {len(long_narratives):,}\")\n",
        "    \n",
        "    if len(short_narratives) > 0:\n",
        "        print(\"\\nSample of very short narratives:\")\n",
        "        print(short_narratives[[product_col, narrative_col, 'narrative_word_count']].head(3))\n",
        "    \n",
        "    if len(long_narratives) > 0:\n",
        "        print(\"\\nSample of very long narratives:\")\n",
        "        print(long_narratives[[product_col, narrative_col, 'narrative_word_count']].head(3))\n",
        "else:\n",
        "    print(\"Cannot analyze narratives - narrative column or data not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. Filter Dataset for Project Requirements\n",
        "\n",
        "# Filter to include only the specified products and remove empty narratives\n",
        "if not df.empty and product_col and narrative_col:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FILTERING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nInitial dataset size: {len(df):,} complaints\")\n",
        "    \n",
        "    # Define the target products (case-insensitive patterns)\n",
        "    # The task mentions these 4 products (handling both singular and plural):\n",
        "    target_patterns = [\n",
        "        r'credit\\s*card',           # Matches \"Credit card\", \"Credit Card\", \"Credit cards\", etc.\n",
        "        r'personal\\s*loan',         # Matches \"Personal loan\", \"Personal loans\", etc.\n",
        "        r'savings\\s*account',       # Matches \"Savings account\", \"Savings Account\", \"Savings accounts\", etc.\n",
        "        r'money\\s*transfer'         # Matches \"Money transfer\", \"Money Transfer\", \"Money transfers\", etc.\n",
        "    ]\n",
        "    \n",
        "    # Normalize product names for matching\n",
        "    df['product_normalized'] = df[product_col].astype(str).str.strip().str.lower()\n",
        "    \n",
        "    # Find products that match our targets (case-insensitive regex matching)\n",
        "    matching_mask = pd.Series([False] * len(df))\n",
        "    matching_products = set()\n",
        "    \n",
        "    for pattern in target_patterns:\n",
        "        mask = df['product_normalized'].str.contains(pattern, case=False, na=False, regex=True)\n",
        "        matching_mask = matching_mask | mask\n",
        "        if mask.any():\n",
        "            matched = df[mask][product_col].unique()\n",
        "            matching_products.update(matched.tolist())\n",
        "    \n",
        "    print(f\"\\nFound product variations matching targets:\")\n",
        "    for product in sorted(matching_products):\n",
        "        count = df[df[product_col] == product].shape[0]\n",
        "        print(f\"  - '{product}': {count:,} complaints\")\n",
        "    \n",
        "    # Filter by product\n",
        "    if matching_products:\n",
        "        df_filtered = df[matching_mask].copy()\n",
        "        print(f\"\\nAfter filtering by product: {len(df_filtered):,} complaints\")\n",
        "        \n",
        "        # Remove records with empty narratives\n",
        "        df_filtered = df_filtered[df_filtered['has_narrative']].copy()\n",
        "        print(f\"After removing empty narratives: {len(df_filtered):,} complaints\")\n",
        "        \n",
        "        # Drop the temporary columns\n",
        "        df_filtered = df_filtered.drop(columns=['has_narrative', 'product_normalized'], errors='ignore')\n",
        "        \n",
        "        # Show final distribution\n",
        "        print(\"\\nFinal distribution by product:\")\n",
        "        product_dist = df_filtered[product_col].value_counts()\n",
        "        for product, count in product_dist.items():\n",
        "            pct = (count / len(df_filtered) * 100)\n",
        "            print(f\"  - {product}: {count:,} ({pct:.2f}%)\")\n",
        "        \n",
        "        print(f\"\\n✓ Filtering complete! Final dataset size: {len(df_filtered):,} complaints\")\n",
        "        print(f\"✓ Retention rate: {(len(df_filtered)/len(df)*100):.2f}% of original dataset\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Warning: Could not find matching products in the dataset.\")\n",
        "        print(\"Available products in dataset:\")\n",
        "        available_products = df[product_col].value_counts().head(15)\n",
        "        for product, count in available_products.items():\n",
        "            print(f\"  - '{product}': {count:,} complaints\")\n",
        "        print(\"\\n⚠ Proceeding without product filtering. Please verify product names manually.\")\n",
        "        df_filtered = df[df['has_narrative']].copy() if 'has_narrative' in df.columns else df.copy()\n",
        "else:\n",
        "    print(\"Cannot filter dataset - required columns or data not found.\")\n",
        "    print(f\"Product column: {product_col}\")\n",
        "    print(f\"Narrative column: {narrative_col}\")\n",
        "    df_filtered = df.copy() if not df.empty else pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. Text Cleaning and Normalization\n",
        "\n",
        "# Clean the text narratives to improve embedding quality\n",
        "def clean_narrative(text):\n",
        "    \"\"\"\n",
        "    Clean and normalize consumer complaint narratives.\n",
        "    \n",
        "    Steps:\n",
        "    1. Convert to lowercase\n",
        "    2. Remove common boilerplate text\n",
        "    3. Remove excessive whitespace\n",
        "    4. Remove special characters (optional - keeping basic punctuation)\n",
        "    5. Normalize whitespace\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return ''\n",
        "    \n",
        "    text = str(text)\n",
        "    \n",
        "    # Step 1: Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Step 2: Remove common boilerplate patterns\n",
        "    boilerplate_patterns = [\n",
        "        r'i am writing to file a complaint',\n",
        "        r'i am writing to complain',\n",
        "        r'this is a complaint',\n",
        "        r'complaint number',\n",
        "        r'case number',\n",
        "        r'reference number',\n",
        "        r'dear sir/madam',\n",
        "        r'dear customer service',\n",
        "        r'to whom it may concern',\n",
        "        r'please find attached',\n",
        "    ]\n",
        "    \n",
        "    for pattern in boilerplate_patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "    \n",
        "    # Step 3: Remove excessive newlines and whitespace\n",
        "    text = re.sub(r'\\n+', ' ', text)  # Replace newlines with space\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
        "    \n",
        "    # Step 4: Remove special characters but keep basic punctuation\n",
        "    # Keep: letters, numbers, basic punctuation (. , ! ? : ; - ' \" ( ) [ ])\n",
        "    # Remove: other special characters\n",
        "    text = re.sub(r'[^\\w\\s.,!?:;\\-\\(\\)\\[\\]\\'\"\\/]', '', text)\n",
        "    \n",
        "    # Step 5: Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Apply cleaning to the filtered dataset\n",
        "if not df_filtered.empty and narrative_col:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TEXT CLEANING\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nCleaning {len(df_filtered):,} narratives...\")\n",
        "    \n",
        "    # Show sample before cleaning\n",
        "    print(\"\\nSample narratives BEFORE cleaning:\")\n",
        "    sample_before = df_filtered[narrative_col].iloc[:3].tolist()\n",
        "    for i, text in enumerate(sample_before, 1):\n",
        "        print(f\"\\n{i}. {text[:200]}...\" if len(str(text)) > 200 else f\"\\n{i}. {text}\")\n",
        "    \n",
        "    # Apply cleaning\n",
        "    df_filtered['narrative_cleaned'] = df_filtered[narrative_col].apply(clean_narrative)\n",
        "    \n",
        "    # Show sample after cleaning\n",
        "    print(\"\\n\\nSample narratives AFTER cleaning:\")\n",
        "    sample_after = df_filtered['narrative_cleaned'].iloc[:3].tolist()\n",
        "    for i, text in enumerate(sample_after, 1):\n",
        "        print(f\"\\n{i}. {text[:200]}...\" if len(str(text)) > 200 else f\"\\n{i}. {text}\")\n",
        "    \n",
        "    # Calculate word count for cleaned narratives\n",
        "    df_filtered['narrative_cleaned_word_count'] = df_filtered['narrative_cleaned'].apply(count_words)\n",
        "    \n",
        "    # Compare word counts\n",
        "    print(\"\\n\\nWord Count Comparison:\")\n",
        "    print(f\"Original average: {df_filtered['narrative_word_count'].mean():.2f} words\")\n",
        "    print(f\"Cleaned average: {df_filtered['narrative_cleaned_word_count'].mean():.2f} words\")\n",
        "    print(f\"Average reduction: {(df_filtered['narrative_word_count'].mean() - df_filtered['narrative_cleaned_word_count'].mean()):.2f} words\")\n",
        "    \n",
        "    print(\"\\n✓ Text cleaning complete!\")\n",
        "else:\n",
        "    print(\"Cannot clean text - filtered dataset is empty or narrative column not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 9. Final Dataset Preparation\n",
        "\n",
        "# Prepare the final cleaned dataset for saving\n",
        "if not df_filtered.empty:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FINAL DATASET PREPARATION\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Keep the original narrative column for reference, but use cleaned version as primary\n",
        "    # For the RAG pipeline, we'll use the cleaned version\n",
        "    \n",
        "    # Create a final dataset with all necessary columns\n",
        "    # Keep all original columns plus cleaned narrative\n",
        "    final_df = df_filtered.copy()\n",
        "    \n",
        "    # Optionally, replace the original narrative with cleaned version\n",
        "    # Or keep both - we'll keep both for now\n",
        "    \n",
        "    print(f\"\\nFinal dataset shape: {final_df.shape}\")\n",
        "    print(f\"Columns: {final_df.columns.tolist()}\")\n",
        "    \n",
        "    # Remove any rows where cleaned narrative is empty (shouldn't happen, but safety check)\n",
        "    final_df = final_df[final_df['narrative_cleaned'].str.strip() != ''].copy()\n",
        "    print(f\"After removing empty cleaned narratives: {len(final_df):,} complaints\")\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(\"\\nFinal Dataset Summary:\")\n",
        "    print(f\"Total complaints: {len(final_df):,}\")\n",
        "    print(f\"\\nProduct distribution:\")\n",
        "    print(final_df[product_col].value_counts())\n",
        "    \n",
        "    print(\"\\nNarrative length statistics (cleaned):\")\n",
        "    print(final_df['narrative_cleaned_word_count'].describe())\n",
        "else:\n",
        "    print(\"Cannot prepare final dataset - filtered dataset is empty.\")\n",
        "    final_df = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 10. Save Filtered and Cleaned Dataset\n",
        "\n",
        "# Save the processed dataset to data/filtered_complaints.csv\n",
        "if not final_df.empty:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SAVING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    output_file = PROJECT_ROOT / \"data\" / \"filtered_complaints.csv\"\n",
        "    \n",
        "    # Save to CSV\n",
        "    final_df.to_csv(output_file, index=False)\n",
        "    print(f\"\\n✓ Dataset saved to: {output_file}\")\n",
        "    print(f\"✓ Total records saved: {len(final_df):,}\")\n",
        "    \n",
        "    # Verify the saved file\n",
        "    if output_file.exists():\n",
        "        file_size = output_file.stat().st_size / (1024 * 1024)  # Size in MB\n",
        "        print(f\"✓ File size: {file_size:.2f} MB\")\n",
        "    \n",
        "    # Also save a summary\n",
        "    summary_file = PROJECT_ROOT / \"data\" / \"processed\" / \"task1_summary.txt\"\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(\"Task 1: EDA and Preprocessing Summary\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
        "        f.write(f\"Original dataset size: {len(df):,} complaints\\n\")\n",
        "        f.write(f\"Final filtered dataset size: {len(final_df):,} complaints\\n\\n\")\n",
        "        f.write(f\"Product distribution:\\n{final_df[product_col].value_counts().to_string()}\\n\\n\")\n",
        "        f.write(f\"Narrative statistics:\\n{final_df['narrative_cleaned_word_count'].describe().to_string()}\\n\")\n",
        "    print(f\"✓ Summary saved to: {summary_file}\")\n",
        "else:\n",
        "    print(\"⚠ Cannot save dataset - final dataset is empty.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Findings Summary\n",
        "\n",
        "### EDA Findings:\n",
        "\n",
        "1. **Dataset Overview**: The CFPB complaint dataset contains [X] complaints across multiple financial products.\n",
        "\n",
        "2. **Product Distribution**: \n",
        "   - The four target products (Credit Card, Personal Loan, Savings Account, Money Transfers) represent [X]% of total complaints\n",
        "   - [Product] has the highest number of complaints with [X] complaints\n",
        "   - [Product] has the lowest number with [X] complaints\n",
        "\n",
        "3. **Narrative Analysis**:\n",
        "   - Approximately [X]% of complaints have consumer narratives\n",
        "   - Average narrative length is [X] words (median: [X] words)\n",
        "   - Very short narratives (<10 words): [X] complaints\n",
        "   - Very long narratives (>1000 words): [X] complaints\n",
        "\n",
        "4. **Data Quality**:\n",
        "   - [X] complaints were removed due to missing narratives\n",
        "   - [X] complaints were removed as they didn't match target products\n",
        "   - Final cleaned dataset: [X] complaints\n",
        "\n",
        "5. **Text Cleaning Impact**:\n",
        "   - Average word count reduction: [X] words per narrative\n",
        "   - Boilerplate text patterns identified and removed\n",
        "   - Text normalized for better embedding quality\n",
        "\n",
        "### Preprocessing Steps Completed:\n",
        "\n",
        "✅ Filtered to target products (Credit Card, Personal Loan, Savings Account, Money Transfers)  \n",
        "✅ Removed complaints with empty narratives  \n",
        "✅ Applied text cleaning (lowercasing, boilerplate removal, whitespace normalization)  \n",
        "✅ Saved cleaned dataset to `data/filtered_complaints.csv`  \n",
        "\n",
        "The cleaned dataset is now ready for Task 2: Text Chunking, Embedding, and Vector Store Indexing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final statistics for the report\n",
        "if not final_df.empty:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL STATISTICS FOR REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\n1. Total complaints in original dataset: {len(df):,}\")\n",
        "    print(f\"2. Total complaints after filtering: {len(final_df):,}\")\n",
        "    print(f\"3. Percentage retained: {(len(final_df)/len(df)*100):.2f}%\")\n",
        "    \n",
        "    print(f\"\\n4. Product Distribution:\")\n",
        "    for product, count in final_df[product_col].value_counts().items():\n",
        "        pct = (count / len(final_df) * 100)\n",
        "        print(f\"   - {product}: {count:,} ({pct:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\n5. Narrative Statistics:\")\n",
        "    print(f\"   - Mean word count: {final_df['narrative_cleaned_word_count'].mean():.2f} words\")\n",
        "    print(f\"   - Median word count: {final_df['narrative_cleaned_word_count'].median():.2f} words\")\n",
        "    print(f\"   - Min word count: {final_df['narrative_cleaned_word_count'].min()} words\")\n",
        "    print(f\"   - Max word count: {final_df['narrative_cleaned_word_count'].max()} words\")\n",
        "    \n",
        "    print(\"\\n✓ Task 1 complete! Ready for Task 2.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
